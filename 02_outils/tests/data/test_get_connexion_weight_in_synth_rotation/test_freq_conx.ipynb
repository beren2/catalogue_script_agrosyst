{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTREPOT_PATH = '~/Bureau/Datagrosyst/data_entrepot_&_outils/'\n",
    "\n",
    "# synth = pd.read_csv(ENTREPOT_PATH + 'synthetise.csv')\n",
    "conx = pd.read_csv(ENTREPOT_PATH + 'connection_synthetise.csv')\n",
    "noeud = pd.read_csv(ENTREPOT_PATH + 'noeuds_synthetise.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conx = conx[['id', 'frequence_source','culture_absente','source_noeuds_synthetise_id','cible_noeuds_synthetise_id']]\\\n",
    "    .rename(columns={'id' : 'conx_id',\n",
    "                     'frequence_source' : 'freq',\n",
    "                     'culture_absente' : 'abs',\n",
    "                     'source_noeuds_synthetise_id' : 'nd_prec',\n",
    "                     'cible_noeuds_synthetise_id' : 'nd_suiv'})\n",
    "noeud = noeud[['id', 'rang', 'fin_cycle','memecampagne_noeudprecedent', 'synthetise_id']]\\\n",
    "    .rename(columns={'id' : 'nd_id',\n",
    "                     'fin_cycle' : 'end',\n",
    "                     'rang' : 'rang',\n",
    "                     'memecampagne_noeudprecedent' : 'sameyear',\n",
    "                     'synthetise_id' : 'synth_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_if_good_schema = [\n",
    "    # En dessous : ne doit pas passer le filtre\n",
    "\n",
    "    'fr.inra.agrosyst.api.entities.practiced.PracticedSystem_6d19dd88-dcc4-4a41-8d45-068fbcd4f88c', # end node qui continue\n",
    "    'fr.inra.agrosyst.api.entities.practiced.PracticedSystem_df56abb7-1a39-4686-bb03-cd85441fc9f6', # commence par 2 noeud\n",
    "    'fr.inra.agrosyst.api.entities.practiced.PracticedSystem_edf4f144-e853-453b-b4e7-fef8f3ebdb99', # trou dans le chemin\n",
    "    \n",
    "    'fr.inra.agrosyst.api.entities.practiced.PracticedSystem_dc9eca9f-8b72-4f1e-b791-790ca4488a60', # full sameyear + qlq abs\n",
    "    # devrait etre ok..... Oops monsieur a oublié de sommer les sorties du fictif à 100% (99.9%)\n",
    "\n",
    "    # En dessous : Doit passer le 1° filtre\n",
    "\n",
    "    'fr.inra.agrosyst.api.entities.practiced.PracticedSystem_592b3792-8ad0-4213-ab98-a948f444ed04', # chemin entierment absent. PASSE CE PREMIER FILTRE ! ne passera pas celui après la fonction principale !!!!!!!!!!!!!!!!!!!!!!\n",
    "    'fr.inra.agrosyst.api.entities.practiced.PracticedSystem_41ae9d22-5515-44d8-9a7f-8254c42149eb', # ok passe le filtre, classique\n",
    "    'fr.inra.agrosyst.api.entities.practiced.PracticedSystem_445dd407-58f6-403e-8dd6-8352166a0131', # trou dans le chemin MAIS sameyear dans les trou donc ok !\n",
    "\n",
    "    # En dessous : ok mais exemple pour la fonction principale\n",
    "    \n",
    "    'fr.inra.agrosyst.api.entities.practiced.PracticedSystem_d4a1b64c-afa0-440f-92e1-30a483871ab4', # bcp de sameyear\n",
    "    'fr.inra.agrosyst.api.entities.practiced.PracticedSystem_61f75804-3823-4fae-9ce1-82bfa3d7e41e', # CBO avec abs dans sameyear\n",
    "    'fr.inra.agrosyst.api.entities.practiced.PracticedSystem_91611752-8cd2-42f1-b19f-97186597ab64', # CBO avec abs dans sameyear\n",
    "    'fr.inra.agrosyst.api.entities.practiced.PracticedSystem_2f68d851-cb30-402f-bfc6-b0abf37c49a8' # classique avec des abs et sameyear mais pas intriqués\n",
    "]\n",
    "\n",
    "\n",
    "noeud = noeud.loc[noeud['synth_id'].isin(check_if_good_schema)]\n",
    "conx = conx.loc[(conx['nd_prec'].isin(noeud['nd_id'])) | (conx['nd_suiv'].isin(noeud['nd_id'])),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = conx.merge(noeud[['nd_id','rang','end']].add_suffix('_prec'), left_on='nd_prec', right_on='nd_id_prec')\\\n",
    "    .drop('nd_id_prec', axis=1)\n",
    "df = df.merge(noeud.add_suffix('_suiv'), left_on='nd_suiv', right_on='nd_id_suiv').\\\n",
    "    rename(columns={'synth_id_suiv' : 'synth_id'})\\\n",
    "        .drop('nd_id_suiv', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_node_rank0(dfgrp):\n",
    "    return len(dfgrp.loc[dfgrp['rang']==0,'rang'])\n",
    "\n",
    "def number_node_end(dfgrp):\n",
    "    return len(dfgrp.loc[dfgrp['end']=='t','end'])\n",
    "\n",
    "def noeud_end_on_rank0(dfgrp):\n",
    "    return any(dfgrp.loc[dfgrp['end']=='t','rang']==0)\n",
    "\n",
    "noeud_test = noeud.groupby('synth_id').apply(\n",
    "    lambda dfgrp: pd.Series({\n",
    "        'nb_noeud_de_rang1': number_node_rank0(dfgrp),\n",
    "        'nb_noeud_finaux' : number_node_end(dfgrp),\n",
    "        'noeud_finaux_en_rang1' : noeud_end_on_rank0(dfgrp)\n",
    "        }), include_groups=False).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# noeuds sans connexions avant ou après\n",
    "print(len(noeud.loc[~((noeud['nd_id'].isin(conx['nd_prec'])) & (noeud['nd_id'].isin(conx['nd_suiv']))),'synth_id']))\n",
    "# on tague les synthétisé sans aucun noeud de rang 1\n",
    "print(len(noeud_test.loc[noeud_test['nb_noeud_de_rang1']==0,'nb_noeud_de_rang1']))\n",
    "# on tague les synthétisé avec plus de 1 noeud de rang 1\n",
    "print(len(noeud_test.loc[noeud_test['nb_noeud_de_rang1']>1,'nb_noeud_de_rang1']))\n",
    "# on tague les synthétisé avec aucun noeud terminal\n",
    "print(len(noeud_test.loc[noeud_test['nb_noeud_finaux']==0,'nb_noeud_finaux']))\n",
    "# on tague les synthétisé avec au moins 1 noeud terminal en rang 1\n",
    "print(len(noeud_test.loc[noeud_test['noeud_finaux_en_rang1']>0,'noeud_finaux_en_rang1']))\n",
    "\n",
    "list_bad_synth = list(noeud_test.loc[(noeud_test['nb_noeud_de_rang1']==0) | \\\n",
    "                                (noeud_test['nb_noeud_de_rang1']>1) | \\\n",
    "                                (noeud_test['nb_noeud_finaux']==0) | \\\n",
    "                                (noeud_test['noeud_finaux_en_rang1']>0)].index)\n",
    "list_bad_synth = list_bad_synth + list(noeud.loc[~((noeud['nd_id'].isin(conx['nd_prec'])) & (noeud['nd_id'].isin(conx['nd_suiv']))),'synth_id'])\n",
    "\n",
    "# Supprimer les rang entierement vide\n",
    "def get_hole_in_rotation(series):\n",
    "    ranks_theo = pd.Series(range(min(series)+1, max(series)))\n",
    "    if all(ranks_theo.isin(series)) == False :\n",
    "        return list(ranks_theo.loc[~(ranks_theo.isin(series))])\n",
    "\n",
    "test_empty_rank = noeud[['rang','synth_id']].groupby('synth_id').agg(get_hole_in_rotation).reset_index()\n",
    "test_empty_rank = test_empty_rank.rename(columns={'rang':'empty_ranks'})\n",
    "test_empty_rank = test_empty_rank.loc[test_empty_rank['empty_ranks'].notna()] # 75 synth avec rang full vide\n",
    "\n",
    "list_bad_synth = list_bad_synth + list(test_empty_rank['synth_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# somme de sortie du noeuf doit faire 100%\n",
    "def concat_unique_sorted_txt(series):\n",
    "    cleaned = series.dropna().unique().copy()\n",
    "    return '_&_'.join(sorted(cleaned))\n",
    "\n",
    "test_100_exit = df[['synth_id','freq','nd_prec']].groupby('nd_prec').agg({\n",
    "    'synth_id' : concat_unique_sorted_txt,\n",
    "    'freq' : 'sum'})\n",
    "test_100_exit = test_100_exit.loc[test_100_exit['freq'] != 100,]\n",
    "\n",
    "list_bad_synth = list_bad_synth + list(test_100_exit['synth_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fr.inra.agrosyst.api.entities.practiced.PracticedSystem_df56abb7-1a39-4686-bb03-cd85441fc9f6',\n",
       " 'fr.inra.agrosyst.api.entities.practiced.PracticedSystem_dc9eca9f-8b72-4f1e-b791-790ca4488a60']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_bad_synth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pas de connexion qui sont à 0% (apparament mais je sias pas pk)\n",
    "# on fait attention aux connexions inférieurs à 0.5%\n",
    "\n",
    "test_cnx_0 = df.loc[df['freq'] < 0.5,].copy()\n",
    "\n",
    "list_bad_synth = list_bad_synth + list(test_cnx_0['synth_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noeud suivant est forcement sur le rang suivant (pas de 'trou') \n",
    "# attention si le rang qui a un trou dans le chemin n'est fait que de culture dérobée et que le noeud suivant n'est pas une dérobée\n",
    "\n",
    "# OU au moins sur un rang supérieur \n",
    "# test = df.loc[(df['rang_prec'] > df['rang_suiv'] ) & (df['end_prec'] == 'f'),] # apparement impossible vu que 0 fois\n",
    "\n",
    "test_hole_in_path = df.loc[(df['rang_prec'] != (df['rang_suiv']-1) ) & (df['end_prec'] == 'f'),].copy()\n",
    "test_hole_in_path['empty_rank_are_catch_crop'] = ''\n",
    "\n",
    "for idx, row in test_hole_in_path.iterrows() :\n",
    "    empty_rank_list = list(range(row['rang_prec']+1, row['rang_suiv']))\n",
    "    if list(noeud.loc[(noeud['synth_id']==row['synth_id']) & (noeud['rang'].isin(empty_rank_list)), 'sameyear']=='t') == []:\n",
    "        test_hole_in_path.at[idx,'empty_rank_are_catch_crop'] = 'empty_rank'\n",
    "    elif (row['sameyear_suiv'] == 'f') & (all(list(noeud.loc[(noeud['synth_id']==row['synth_id']) & (noeud['rang'].isin(empty_rank_list)), 'sameyear'] == 't'))) & (all(list(noeud.loc[(noeud['synth_id']==row['synth_id']) & (noeud['rang']==row['rang_suiv']), 'sameyear'] == 'f'))) \\\n",
    "    | \\\n",
    "    (row['sameyear_suiv'] == 'f') & (all(list(noeud.loc[(noeud['synth_id']==row['synth_id']) & (noeud['rang'].isin(empty_rank_list)), 'sameyear'] == 'f'))) & (all(list(noeud.loc[((noeud['synth_id']==row['synth_id']) & (noeud['rang']==row['rang_suiv'])) & (noeud['nd_id']!=row['nd_suiv']), 'sameyear'] == 't'))) :\n",
    "        test_hole_in_path.at[idx,'empty_rank_are_catch_crop'] = 'ok'\n",
    "    else :\n",
    "        test_hole_in_path.at[idx,'empty_rank_are_catch_crop'] = 'hole_in_path'\n",
    "\n",
    "\n",
    "test_hole_in_path = test_hole_in_path.groupby('synth_id').agg({'empty_rank_are_catch_crop': concat_unique_sorted_txt}).reset_index()\n",
    "\n",
    "list_bad_synth = list_bad_synth + list(test_hole_in_path.loc[test_hole_in_path['empty_rank_are_catch_crop'] != 'ok', 'synth_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noeud terminaux qui ne boucle pas entierement sur un noeud de premier rang\n",
    "end_node_continue = list(df.loc[(df['end_prec'] == 't') & df['rang_suiv'] != 0, 'synth_id'])\n",
    "\n",
    "list_bad_synth = list_bad_synth + end_node_continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_bad_synth = list(set(list_bad_synth))\n",
    "\n",
    "cx = df.loc[~(df['synth_id'].isin(list_bad_synth))].set_index('conx_id').copy()\n",
    "nd = noeud.loc[~(noeud['synth_id'].isin(list_bad_synth))].set_index('nd_id').copy()\n",
    "\n",
    "list_good_synth = tuple(set(nd['synth_id']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fr.inra.agrosyst.api.entities.practiced.PracticedSystem_df56abb7-1a39-4686-bb03-cd85441fc9f6',\n",
       " 'fr.inra.agrosyst.api.entities.practiced.PracticedSystem_6d19dd88-dcc4-4a41-8d45-068fbcd4f88c',\n",
       " 'fr.inra.agrosyst.api.entities.practiced.PracticedSystem_9791de7e-ccc4-4ecb-9cbb-a93273022ce8',\n",
       " 'fr.inra.agrosyst.api.entities.practiced.PracticedSystem_edf4f144-e853-453b-b4e7-fef8f3ebdb99',\n",
       " 'fr.inra.agrosyst.api.entities.practiced.PracticedSystem_dc9eca9f-8b72-4f1e-b791-790ca4488a60']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_bad_synth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fr.inra.agrosyst.api.entities.practiced.PracticedSystem_91611752-8cd2-42f1-b19f-97186597ab64',\n",
       " 'fr.inra.agrosyst.api.entities.practiced.PracticedSystem_445dd407-58f6-403e-8dd6-8352166a0131',\n",
       " 'fr.inra.agrosyst.api.entities.practiced.PracticedSystem_41ae9d22-5515-44d8-9a7f-8254c42149eb',\n",
       " 'fr.inra.agrosyst.api.entities.practiced.PracticedSystem_61f75804-3823-4fae-9ce1-82bfa3d7e41e',\n",
       " 'fr.inra.agrosyst.api.entities.practiced.PracticedSystem_d4a1b64c-afa0-440f-92e1-30a483871ab4',\n",
       " 'fr.inra.agrosyst.api.entities.practiced.PracticedSystem_2f68d851-cb30-402f-bfc6-b0abf37c49a8',\n",
       " 'fr.inra.agrosyst.api.entities.practiced.PracticedSystem_592b3792-8ad0-4213-ab98-a948f444ed04')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_good_synth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour trouver tous les chemins possibles avec DFS (itératif pour éviter la récursion trop profonde)\n",
    "def trouver_chemins(graphe, debut, fins):\n",
    "    stack = [(debut, [])]  # (node actuel, chemin parcouru)\n",
    "    chemins = []\n",
    "\n",
    "    while stack:\n",
    "        node, chemin = stack.pop()\n",
    "        chemin.append(node)\n",
    "\n",
    "        if node in fins:\n",
    "            chemins.append(list(chemin))\n",
    "            continue\n",
    "\n",
    "        for voisin in graphe.get(node, []):\n",
    "            if voisin not in chemin:  # Éviter les cycles\n",
    "                stack.append((voisin, chemin[:]))  # Copie du chemin actuel\n",
    "\n",
    "    return chemins\n",
    "\n",
    "\n",
    "final_data = pd.DataFrame()\n",
    "\n",
    "\n",
    "def process_sy(sy):\n",
    "\n",
    "    # Construire le graphe des connexions sous forme de dictionnaire\n",
    "    graphe = {}\n",
    "    connexions = {}\n",
    "\n",
    "    for idx, row in cx.iterrows():\n",
    "        nd_prec, nd_suiv, conx_id, abs_value, freq = row['nd_prec'], row['nd_suiv'], idx, row['abs'], row['freq']\n",
    "        \n",
    "        if nd_prec not in graphe:\n",
    "            graphe[nd_prec] = []\n",
    "        graphe[nd_prec].append(nd_suiv)\n",
    "\n",
    "        # Stocker les connexions avec leurs IDs et attributs pour un accès rapide\n",
    "        connexions[(nd_prec, nd_suiv)] = {\n",
    "            'conx_id': conx_id,\n",
    "            'abs': abs_value,\n",
    "            'freq': freq\n",
    "        }\n",
    "\n",
    "    # Trouver le premier nœud et les nœuds finaux\n",
    "    first_node = nd.loc[(nd['rang'] == 0) & (nd['synth_id'] == sy)].index.item()\n",
    "    end_nodes = set(nd.loc[(nd['end'] == 't') & (nd['synth_id'] == sy)].index)\n",
    "\n",
    "    # Obtenir tous les chemins\n",
    "    chemins_possibles = trouver_chemins(graphe, first_node, end_nodes)\n",
    "\n",
    "    #  Associer chaque connexion aux chemins où elle apparaît, en comptant abs='t', sameyear='t', et calculant le poids du chemin\n",
    "    df_couples_connexion_chemins = []\n",
    "    df_chemins = []\n",
    "\n",
    "    for chemin in chemins_possibles:\n",
    "\n",
    "        poid_chemin = 1\n",
    "        groupe_id = 0\n",
    "        groupes_sameyear = {}\n",
    "\n",
    "        # Déterminer les groupes sameyear chemin par chemin\n",
    "        for i, node3 in enumerate(chemin):\n",
    "            if i == 0 or nd.loc[node3, 'sameyear'] == 'f':\n",
    "                groupe_id += 1  # Nouveau groupe\n",
    "            groupes_sameyear[node3] = groupe_id  # Associer le nœud à son groupe\n",
    "        # Si le premier noeud est en sameyear on associe son groupe à toutes les connexions qui ont le meme groupe que le dernier neoud\n",
    "        if nd.loc[chemin[0], 'sameyear'] == 't':\n",
    "            list_node_same_grp_as_last = [key for key, value in groupes_sameyear.items() if value == groupes_sameyear[chemin[-1]]]\n",
    "            for key in list_node_same_grp_as_last: groupes_sameyear[key] = groupes_sameyear[chemin[0]]\n",
    "\n",
    "        # Calculer le poids du chemin (produit) et Ajouter les infos de couples connexions/chemins\n",
    "        for i in range(len(chemin) - 1):\n",
    "            nd_prec, nd_suiv = chemin[i], chemin[i + 1]\n",
    "            if (nd_prec, nd_suiv) in connexions:\n",
    "                poid_chemin *= connexions[(nd_prec, nd_suiv)]['freq'] / 100  # Convertir en probabilité\n",
    "                df_couples_connexion_chemins.append({\n",
    "                    'connexion_id': connexions[(nd_prec, nd_suiv)]['conx_id'],\n",
    "                    'chemin_id': chemin,\n",
    "                    'groupe_sameyear': groupes_sameyear[nd_suiv],  # La connexion prend le groupe du noeud suivant\n",
    "                    'abs': connexions[(nd_prec, nd_suiv)]['abs']\n",
    "                })\n",
    "        # Ajout des connexions des nœuds terminaux vers le premier nœud\n",
    "        if chemin[-1] in end_nodes and (chemin[-1], chemin[0]) in connexions:\n",
    "            poid_chemin *= connexions[(chemin[-1], chemin[0])]['freq'] / 100\n",
    "            df_couples_connexion_chemins.append({\n",
    "                'connexion_id': connexions[(chemin[-1], chemin[0])]['conx_id'],\n",
    "                'chemin_id': chemin,\n",
    "                'groupe_sameyear': groupes_sameyear[chemin[0]],  # La connexion prend le groupe du premier noeud\n",
    "                'abs': connexions[(chemin[-1], chemin[0])]['abs']\n",
    "            })\n",
    "        \n",
    "        # Nombre d'année (soit le nombre de groupe sameyear après avoir enlever les connexions absentes)\n",
    "        nb_annee = len(set([entry['groupe_sameyear'] for entry in df_couples_connexion_chemins if \\\n",
    "                            (entry['abs'] == 'f') & (entry['chemin_id'] == chemin)]))\n",
    "        if nb_annee == 0 : nb_annee = 1\n",
    "        # Ajouter les infos des chemins, dont le poids des chemins après recalcul sans conx absente\n",
    "        df_chemins.append({\n",
    "            'chemin_id': chemin,\n",
    "            'pd_chem' : poid_chemin,\n",
    "            'poid_standard_cnx_wo_abs': poid_chemin / nb_annee,\n",
    "            'synth_id': sy\n",
    "        })\n",
    "\n",
    "    # Convertir en DataFrame\n",
    "    df_chemins = pd.DataFrame(df_chemins)\n",
    "    df_chemins['chemin_id'] = df_chemins['chemin_id'].astype(str)\n",
    "    df_couples_connexion_chemins = pd.DataFrame(df_couples_connexion_chemins)\n",
    "    df_couples_connexion_chemins['chemin_id'] = df_couples_connexion_chemins['chemin_id'].astype(str)\n",
    "    # Merge chemin sur les couples cx_ch\n",
    "    df_couples_connexion_chemins = df_couples_connexion_chemins.merge(df_chemins, on = 'chemin_id', how = 'left')\n",
    "\n",
    "    # Avoir le compte du nombre de connexions active (abs == 'f') ayant le meme chemin ET le meme groupe_sameyear\n",
    "    nb_grp_sameyear_overall = df_couples_connexion_chemins[df_couples_connexion_chemins['abs'] == 'f'].\\\n",
    "        groupby(['chemin_id', 'groupe_sameyear']).size().reset_index(name='count_grp_sameyear_overall')\n",
    "    all_df = df_couples_connexion_chemins.merge(nb_grp_sameyear_overall, on=['chemin_id', 'groupe_sameyear'], how='left')\n",
    "\n",
    "    # Calculer le vrai poids de connexion final (poid_standard_cnx_wo_abs / count_grp_sameyear_overall)\n",
    "    all_df['connexion_freq'] = all_df['poid_standard_cnx_wo_abs'] / all_df['count_grp_sameyear_overall']\n",
    "\n",
    "    # Supprimer le poids de connexion des connexions absentes\n",
    "    all_df.loc[all_df['abs'] == 't','connexion_freq'] = np.nan\n",
    "\n",
    "    return all_df\n",
    "\n",
    "\n",
    "\n",
    "# Utilisation de ProcessPoolExecutor avec 80% des cœurs\n",
    "with ProcessPoolExecutor(max_workers= max(1, int(os.cpu_count() * 0.7)) ) as executor:\n",
    "    results = list(tqdm(executor.map(process_sy, list_good_synth), total=len(list_good_synth)))\n",
    "\n",
    "# Concaténation des résultats\n",
    "final_data = pd.concat(results)\n",
    "\n",
    "test = final_data.copy()\n",
    "test = test[['connexion_freq','synth_id']].groupby('synth_id').sum('connexion_freq')\n",
    "test = test.loc[round(test['connexion_freq'],2) != 1]\n",
    "\n",
    "final_data = final_data.loc[~(final_data['synth_id'].isin(test.index))]\n",
    "\n",
    "final_data_conx_level = final_data[['connexion_id','connexion_freq','abs','synth_id']].groupby('connexion_id').agg({\n",
    "    'connexion_freq' : lambda x : np.nan if all(x.isna()) else sum(x.dropna()),\n",
    "    'abs' : lambda x : x.unique().item(), # Pas exportée\n",
    "    'synth_id' : lambda x : x.unique().item() # Pas exportée\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv('/home/administrateur/Bureau/test_conx_chem.csv', index = False)\n",
    "final_data_conx_level.to_csv('/home/administrateur/Bureau/test_weigth_conx.csv', index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "catalogue_script_agrosyst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
